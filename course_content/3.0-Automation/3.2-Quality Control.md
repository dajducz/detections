# 3.2 :: Quality Control

Now that we've got rules automatically being pulled from our GitHub repo and being dropped directly into our production environment, we can take a breather and go touch some grass, right? Nothing has ever gone wrong with automating unchecked code directly to production.

Okay... if you really *want* to make sure that your rules are validated and functional, we have a few tools available to us to ensure only high quality detections make it into your SIEM.

## Workflows

GitHub's automated workflow system allows us to run jobs as defined in the YAML files that are defined in the `.github/workflows` folder of our repo. These jobs can perform a variety of tasks and we will use them to check every pull request in our repo to ensure only high quality detections make it past our filters. If you wanted to you could even use a workflow to replace the actions we built in our previous chapter, the automation of pushing our converted rules directly to our SIEM.

I will cover some of the basics but you can learn more about GitHub workflows [here](https://docs.github.com/en/actions/using-workflows/about-workflows).

At a high level a workflow waits for an event to trigger the workflow and will then assign a runner to execute the tasks defined for the associated job. These tasks are often scripts that execute in a sequential order.


Let's walk through one of the workflows we'll be using. Open up the `.github/workflows` folder and modify the `sigma-test.yml` file with the following code:

```yaml
# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Sigma Rule Tests

on:
  push:
    branches:
      - "*"
    paths:
      - ".github/workflows/sigma-test.yml"
      - "deprecated/**.yml"
      - "rules/**.yml"
      - "tests/test_logsource.py"
      - "tests/test_rules.py"
  pull_request:
    branches:
      - main
    paths:
      - ".github/workflows/sigma-test.yml"
      - "deprecated/**.yml"
      - "rules/**.yml"
      - "tests/test_logsource.py"
      - "tests/test_rules.py"

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  yamllint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: yaml-lint
      uses: ibiqlik/action-yamllint@v3

  test-sigma-logsource:
    runs-on: ubuntu-latest
    needs: yamllint
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: 3.11
    - name: Test Sigma logsource
      run: |
        pip install PyYAML colorama
        python tests/test_logsource.py

  test-sigma:
    runs-on: ubuntu-latest
    needs: test-sigma-logsource
    steps:
    - uses: actions/checkout@v4
      with:
        submodules: true
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: 3.11
    - name: Install dependencies
      run: |
        # pip install sigma-cli~=0.7.1
        pip install sigma-cli
    - name: Test Sigma Rule Syntax
      run: |
        sigma check --fail-on-error --fail-on-issues --validation-config tests/sigma_cli_conf.yml rules*
    - name: Test Sigma Rules
      run: |
        pip install PyYAML colorama
        python tests/test_rules.py
```

In this workflow we are defining the triggers in the `on:` section with the `push:` and `pull_request:` keywords. Then we are defining the `jobs:` that we'll be running. Looking closer at one of the jobs:

```yaml
  yamllint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: yaml-lint
      uses: ibiqlik/action-yamllint@v3
```

we have the name of the job, `yamllint`, the system or image it will run on, `ubuntu-latest`, and the steps that job will take. The steps can `use` GitHub actions and in this case we're using `actions/checkout@v4` which will checkout the repository onto our runner, allowing the following scripts to have access to that code. Then we name our script `yaml-lint` and use another action from [another GitHub action library](https://github.com/ibiqlik/action-yamllint) to actually do perform an action.

In this case we are running a yaml linter which is a tool that will make sure that the syntax and formatting in your Sigma rules matches the agreed upon standard of the yaml format. There are specific checks that we modify from the standards for Sigma rules and those are defined in the `.yamllint` file in the root of our GitHub repo.

The next two jobs that we run in this workflow are Python scripts to validate that we are using proper logsources and field names as defined in the script `/tests/test_logsource.py` (with the helper file `/tests/logsource.json`) and then a test to validate that our Sigma rule will properly convert using sigma-cli and the script `/tests/test_rules.py`.

Before we continue, open up `sigma-validation.yml` and modify the `branches:` keys to be `"*"` for the `push:` section and `"main"` for the `pull_request:` section.

## Workflows In Action

Let's try this out. If you've already forked this repo into your own account, open up the `3.2-quality_control` branch and push the changes you made to your workflows here. Now, open up the rules folder and make a file named `test.yml` and put in any sigma rule that you like from the public repo. I'll use this one:

```yaml
title: Chromium Browser Headless Execution To Mockbin Like Site
id: 1c526788-0abe-4713-862f-b520da5e5316
status: experimental
description: Detects the execution of a Chromium based browser process with the "headless" flag and a URL pointing to the mockbin.org service (which can be used to exfiltrate data).
references:
    - https://www.zscaler.com/blogs/security-research/steal-it-campaign
author: X__Junior (Nextron Systems)
date: 2023-09-11
tags:
    - attack.execution
logsource:
    product: windows
    category: process_creation
detection:
    selection_img:
        Image|endswith:
            - '\brave.exe'
            - '\chrome.exe'
            - '\msedge.exe'
            - '\opera.exe'
            - '\vivaldi.exe'
    selection_headless:
        CommandLine|contains: '--headless'
    selection_url:
        CommandLine|contains:
            - '://run.mocky'
            - '://mockbin'
    condition: all of selection_*
falsepositives:
    - Unknown
level: high
```

GitHub will now prompt you to make a pull request with these changes. Follow this prompt and the following steps until your pull request is created. Take a moment to get a glass of water and when you come back you will see that your workflow checks were performed automatically. If you followed these steps correctly, you should see an error on the `Sigma Rule Tests` check. To see why it failed, click the `Details` button and you will see two errors of why this rule failed to pass:

```
Validation issue summary:
+-------+----------------------+----------+----------------------------------------------+
| Count | Issue                | Severity | Description                                  |
+-------+----------------------+----------+----------------------------------------------+
| 1     | FilenameLenghIssue   | HIGH     | Rule filename is too short or long           |
| 1     | FilenameSigmahqIssue | HIGH     | Rule filemane doesn't match SigmaHQ standard |
+-------+----------------------+----------+----------------------------------------------+
```

This is an easy fix! We just need to modify the rule name to match the rule name format that the SigmaHQ community has agreed upon. Because this is just a demonstration we can just ignore these error messages.

Delete the `test.yml` file from this branch. At the end of this chapter we will merge the quality control branch with your main branch, but we have one more topic to discuss.

## Peer Review

Validating your rules with automated tools is only half the picture for quality control. For now (and hopefully forever), a human element is still required in the detection engineering process. There are just some steps where a human needs to step in and make sure that everything passes muster before a rule can be used in production. 

In your lab this may not matter much because you are the only one using these rules and detections and if something breaks, you can usually just spend an afternoon fixing it. In a corporate environment, things are very different. I recommend turning on GitHub's branch protection rules to force that your automated workflows are required and that at least one person other than the author of the pull request reviews the code before a merge is allowed.

I also recommend setting up some standards on what happens during peer review. Some potential actions a peer reviewer may take are:

- Convert the rule and run it in the SIEM manually to confirm it runs properly.
- Read the review and consider false positives that were not properly filtered and consider possible false negative scenarios where the logic in this rule is insufficient for all attack patterns.
- Write a short list of steps that an analyst who eventually works this detection may take to begin investigation when this shows up in their queue.
- Ensure that the rule runs efficiently and will not consume too many resources on the SIEM
- Validate that the references in the rule link to a useful source

Building these policies early in your detection engineering program may slow progress, but they will pay off.

### Wrapping Up

Let's revisit our pull request. If you turned on some branch protections, you may have to shoulder tap your coworkers and ask them to approve your PR. When they do, we can merge this with our `Main` branch.
