# 3.1 :: A Scheduled Task to Schedule Tasks

We have an important mission in mind: automatically pulling rules from our GitHub repo and publishing them into Splunk as fully functional saved searches. To accomplish this goal, we are going use a cronjob to periodically perform a pull request against our repo, execute SigmaCLI with our pipelines, output the file into our savedsearches.conf, then force the Splunk server to refresh and update the loaded configuration. Up until this point we have done each of these steps manually with a simple script we can automate everything.

Before we build the script, we need to build a service account in Splunk. This account should have limited permissions and the admin account you've been using up until this point would be a blatant violation of the rule of least privileges.

On the Splunk dashboard, navigate to the `Settings` page, select `users and authentication`, next the `Users` page, then the `Create User` feature. Follow the create user wizard and grant this account with privileges no greater than `user` and uncheck the box for "reset password on first login". For optimal security you will want to give this service account the least privilege possible. In this circumstance the account only needs to access the `_reload` API endpoint on the Sigma app but modifying that permission set requires modifying splunk config files and that is out of scope for this course. 

Now that we've got the API user account, we will need to create an environmental variable in our `.bashrc` file named `$SPLUNK_API` so we can obscure our password from the cron task we will create.

Run the following command (replacing the username and password to the values you decided on):
```bash
echo "export SPLUNK_API='api_user_name:ChangeMe!'" >> .bashrc
```
I am using Fish so my command will be a little different:
```bash
set -x SPLUNK_API API_srvc_acct:ChangeMe!
```


With a service account in hand, we can create a bash script that will perform all of the steps we did above in a single go. Copy the below script, add the username and password you created in the curl command, and then save it somewhere meaningful:

```bash
#! /bin/bash

# Pull the git repo. If the repo is private, you'll need to do some extra steps here.
git -C ~/detection-with-sigma/ pull

# Run the sigmaCLI command with your custom pipeline
sigma convert --target 'splunk' --pipeline ~/detection-with-sigma/backend/pipelines/pipeline.yml ~/detection-with-sigma/rules/production/ --skip-unsupported --output ~/detection-with-sigma/backend/savedsearches.conf

# Move the savedsearches.conf output file to the splunk volume
mv ~/detection-with-sigma/backend/savedsearches.conf ~/detection-with-sigma/app/sigma/local/savedsearches.conf

# Hit the refresh endpoint for the app
curl -k -u $SPLUNK_API https://<Splunk Server IP>:8089/servicesNS/-/sigma/saved/searches/_reload
```
These are the same steps we ran in the previous section so they should ~just work~, but if you are unsure you can test. Move a rule into the production folder and then execute the script. Wait a minute and you will see it in the Splunk GUI.

Let's get this script scheduled. Run `crontab -e` and select your favorite editor. Append the following line to the end of the file (with your script location and .sh file name replaced and also where you'd like any logs to be saved):

```
0 0 * * * ~/<script location>/cronjob.sh >> ~/<preferred_log_location>/cron_log.txt
```

And that is it! You've got a scheduled task that, at midnight every day, will pull any new additions to your GitHub repo, automatically convert the rules into `savedsearches.conf` format, and update your Splunk server. **Mission Accomplished.**

