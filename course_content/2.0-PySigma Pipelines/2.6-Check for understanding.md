# 2.6 :: Check for Understanding

To practice building a post processing pipeline, the following scenario will challenge you to build a transformation that will output into `savedsearches.conf` format.

Take a look at the rules available in [rules/testing/section_2.6](https://github.com/The-Taggart-Institute/detection-with-sigma/tree/main/rules/testing/section_2.6). We will build a pipeline that transforms each of these rules and applies conditional logic to output a different configuration based on the severity level of the rules.

Different severity values should be used to define how frequently the searches execute and how far back the search looks against your dataset.

| Severity | Frequency | Earliest Time |
|--|--|--|
| Critical | every 5 min | last 10 min |
| High | every 30m | last hour |
| Medium | every hour | last 2 hours |
| Low | Once at midnight | last 24 hours |

Hints:

- Use the splunk [savedsearches.conf documentation](https://docs.splunk.com/Documentation/Splunk/latest/Admin/Savedsearchesconf#savedsearches.conf.spec) as a reference.
- You will need to use `rule_conditions` on your post-processing transformations. I recommend using the `rule_attribute` condition but this requires you to be on pySigma version 11.9! (As of the time of writing this, sigconverter.io is not on this latest version. Use `pip install pySigma --upgrade` locally)
- The `Severity` in this exercise refers to rules with a matching `level` value.
- Use a tool like [crontab guru](https://crontab.guru) to validate your cron.
- Validate your pipeline with the following command: `sigma convert -p <pipeline> -t splunk rules/testing/section_2.6` (beta.sigconverter.io is no longer sufficient here because you will be converting more than one rule at a time).
