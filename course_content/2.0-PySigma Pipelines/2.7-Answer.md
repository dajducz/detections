# 2.7 :: Answers

There are many ways to accomplish the goal set forth in section 2.6. While we are constrained by the format of savedsearches.conf, we have a lot of freedom on how we will actually present the data in our detection events. Because of the complexity of this task, I wanted to take a moment and describe the choices I made. It is very likely you made different choices and if your pipeline generated a functional output, then your choices were not wrong!

```yaml
# Global win eventlog index
transformations:
    - id: index_condition
      type: add_condition
      conditions:
        index: winevent
      rule_conditions:
        - type: logsource
          product: windows

postprocessing:
    - id: critical_post_processing # CRITICAL severity, runs every 5 minutes with 10 minute lookback
      type: template
      template: |+
            [{{ rule.title }}]
            search = {{ query }} | eval rule_id="{{ rule.id }}", title="{{ rule.title }}", severity="{{rule.level}}"  | collect index=notable_events
            description = "{{ rule.description | replace('\n', ' ') }}"
            cron_schedule = */5 * * * *
            dispatch.earliest_time = -10m
            alert.severity = 6
      rule_conditions:
        - type: rule_attribute
          attribute: level
          value: critical
    - id: high_post_processing # HIGH severity, runs every 30 minutes with 1 hour lookback
      type: template
      template: |+
            [{{ rule.title }}]
            search = {{ query }} | eval rule_id="{{ rule.id }}", title="{{ rule.title }}", severity="{{rule.level}}" | collect index=notable_events
            description = "{{ rule.description | replace('\n', ' ') }}"
            cron_schedule = */30 * * * *
            dispatch.earliest_time = -1h
            alert.severity = 5
      rule_conditions:
        - type: rule_attribute
          attribute: level
          value: high
    - id: medium_post_processing # MEDIUM severity, runs every 60 minutes with 2 hour lookback
      type: template
      template: |+
            [{{ rule.title }}]
            search = {{ query }} | eval rule_id="{{ rule.id }}", title="{{ rule.title }}", severity="{{rule.level}}"  | collect index=notable_events
            description = "{{ rule.description | replace('\n', ' ') }}"
            cron_schedule = */60 * * * *
            dispatch.earliest_time = -2h
            alert.severity = 4
      rule_conditions:
        - type: rule_attribute
          attribute: level
          value: medium
    - id: low_post_processing # LOW severity, uses default values for cron and dispatch, suppress on hostname, filename, action for 24h
      type: template
      template: |+
            [{{ rule.title }}]
            search = {{ query }} | eval rule_id="{{ rule.id }}", title="{{ rule.title }}", severity="{{rule.level}}"  | collect index=notable_events
            description = "{{ rule.description | replace('\n', ' ') }}"
            cron_schedule = 0 0 * * *
            dispatch.earliest_time = -2h
            alert.severity = 3
      rule_conditions:
        - type: rule_attribute
          attribute: level
          value: low

finalizers:
    - type: concat
      prefix: |
            [default]
            dispatch.latest_time = now
            enableSched = 1
            allow_skew = 5m
```

Output:
```
[default]
dispatch.latest_time = now
enableSched = 1
allow_skew = 5m
[AppX Package Installation Attempts Via AppInstaller.EXE]
search = index="winevent" Image="C:\\Program Files\\WindowsApps\\Microsoft.DesktopAppInstaller_*" Image="*\\AppInstaller.exe" | eval rule_id="7cff77e1-9663-46a3-8260-17f2e1aa9d0a", title="AppX Package Installation Attempts Via AppInstaller.EXE", severity="medium"  | collect index=notable_events
description = "Detects DNS queries made by "AppInstaller.EXE". The AppInstaller is the default handler for the "ms-appinstaller" URI. It attempts to load/install a package from the referenced URL "
cron_schedule = */30 * * * *
dispatch.earliest_time = -2h
alert.severity = 4
[Vulnerable WinRing0 Driver Load]
search = index="winevent" ImageLoaded IN ("*\\WinRing0x64.sys", "*\\WinRing0.sys", "*\\WinRing0.dll", "*\\WinRing0x64.dll", "*\\winring00x64.sys") OR Hashes="*IMPHASH=D41FA95D4642DC981F10DE36F4DC8CD7*" OR Imphash="d41fa95d4642dc981f10de36f4dc8cd7" | eval rule_id="1a42dfa6-6cb2-4df9-9b48-295be477e835", title="Vulnerable WinRing0 Driver Load", severity="high" | collect index=notable_events
description = "Detects the load of a signed WinRing0 driver often used by threat actors, crypto miners (XMRIG) or malware for privilege escalation"
cron_schedule = */30 * * * *
dispatch.earliest_time = -1h
alert.severity = 5
[Advanced IP Scanner - File Event]
search = index="winevent" TargetFilename="*\\AppData\\Local\\Temp\\Advanced IP Scanner 2*" | eval rule_id="fed85bf9-e075-4280-9159-fbe8a023d6fa", title="Advanced IP Scanner - File Event", severity="low"  | collect index=notable_events
description = "Detects the use of Advanced IP Scanner. Seems to be a popular tool for ransomware groups."
cron_schedule = 0 0 * * *
dispatch.earliest_time = -2h
alert.severity = 3
[Silence.EDA Detection]
search = index="winevent" ScriptBlockText="*System.Diagnostics.Process*" ScriptBlockText="*Stop-Computer*" ScriptBlockText="*Restart-Computer*" ScriptBlockText="*Exception in execution*" ScriptBlockText="*$cmdargs*" ScriptBlockText="*Close-Dnscat2Tunnel*" ScriptBlockText="*set type=$LookupType`nserver*" ScriptBlockText="*$Command | nslookup 2>&1 | Out-String*" ScriptBlockText="*New-RandomDNSField*" ScriptBlockText="*[Convert]::ToString($SYNOptions, 16)*" ScriptBlockText="*$Session.Dead = $True*" ScriptBlockText="*$Session[\"Driver\"] -eq*" | eval rule_id="3ceb2083-a27f-449a-be33-14ec1b7cc973", title="Silence.EDA Detection", severity="critical"  | collect index=notable_events
description = "Detects Silence EmpireDNSAgent as described in the Group-IP report"
cron_schedule = */5 * * * *
dispatch.earliest_time = -10m
alert.severity = 6
```
The first stanza here is the most important and all others are based upon that. Let's dissect it:

```yaml
postprocessing:
    - id: critical_post_processing # CRITICAL severity, runs every 5 minutes with 10 minute lookback
      type: template
      template: |+
            [{{ rule.title }}]
            search = {{ query }} | eval rule_id="{{ rule.id }}", title="{{ rule.title }}", severity="{{rule.level}}"  | collect index=notable_events
            description = "{{ rule.description | replace('\n', ' ') }}"
            cron_schedule = */5 * * * *
            dispatch.earliest_time = -10m
            alert.severity = 6
      rule_conditions:
        - type: rule_attribute
          attribute: level
          value: critical
```

Line-by-line:
1. Declaration of the postprocessing section of the pipeline
2. ID or title of the transformation object. I used a # to denote a comment to give better details.
3-4. I am using the template type of postprocessing transformation
5. I opted to put the title of the rule in the square brackets. I use ID in other examples, but I personally find this easier to read and understand when I'm in the GUI
6. My search starts with the `{{ query }}` object, then uses `| eval` to add new fields for the rule_id, title, and severity objects, then `| collect` to drop the event into our notable_events index. The collect command is useful here because we can keep a log of triggered detections for future reference or use that index to feed our ticketing systems.
7. The description is pulling the description field from the rule and then using a replace command from Jinja2 to remove newline objects. This is just a trick I've learned to adapt because sometimes Sigma rules will have newlines in the description field that, when converted to savedsearches.conf, break the formatting.
8. `cron_schedule` is set to run every 5 minutes
9. `dispatch.earliest_time` will look at only the last 10 minutes of data in this index. This will create duplicate events! You'll see two every 10 minute block (generally). This is okay and we can use savedsearches.conf's standard for alert suppression if we need to.
10. `alert.severity` is being set `6` for critical according to the savedsearches.conf documentation. This is a nice-to-have.
11. Next I'm moving into the `rule_conditions` section to ensure this only applies to specific rules
12. Defining the `rule_attribute` condition
13+14. Using the key:value pair of `level:critical` to apply this rule only to the critical rules!

Based on this first template, I just needed to make small modifications for the remaining condtions. I changed the cron schedule, the earliest dispatch time, the `alert.severity`, and the `rule_attribute` rule condition to match the expected criteria.

In the finalizer: 
```
            [default]
            dispatch.latest_time = now
            enableSched = 1
            allow_skew = 5m
```
I defined the `[default]` rule conditions to ensure that for every rule converted, the `dispatch.latest_time` would be `now`, I used `enableSched = 1` to make sure that these rules are actually scheduled, and `allow_skew = 5m` to allow Splunk to decide to delay a search up to 5 minutes in case the search head is overloaded with searches.
