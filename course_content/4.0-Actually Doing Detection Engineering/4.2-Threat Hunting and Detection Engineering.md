# 4.2 :: Threat Hunting and Detection Engineering

Threat hunting is a massive topic and could be yet another course in itself. While we won't be digging deep into what threat hunting is and how it is done, I can define the practice of Threat Hunting into a single phase: "Huh, that looks weird."

Being able to look at logs and find the "weird" elements is an art and a skill that you will develop with time and practice. Today we have it easy: we know exactly what is weird because we know exactly what executed in our environment. Now that you've uploaded the logs into your Splunk instances, take the time to dig around a little bit. Make sure the fields were properly extracted and that the data was parsed in a way that makes sense (this should be automatically done by Splunk when you upload them, but I've seen Splunk do some weird things). Pull out some specific fields like the CommandLine fields or the Description fields and see if you can find malicious behavior. If you cannot, that is okay! The point of this exercise is to just get used to looking at logs. We will be implementing some Sigma rules to actually root out the evil in these logs.

## Putting Our Automation To The Test

By now you should have a good understanding of pySigma pipelines and the automation that we've built together to pull rules from your GitHub repo and supply Splunk with the instructions on how to schedule searches. We can now close the loop and see how those scheduled searches perform against live data. Let's start with a review to make sure our infrastructure is prepared for this test.

First things first: confirm that you have the sigma rules we imported in Chapter 3 in the `rules/production` folder.

Next we need to make sure our pipeline is prepared. In the folder `backend/pipelines/` ensure that you have the pipeline.yaml file that we created back in Chapter 2 here. If you changed the name of the index that your data was uploaded to, modify this pipeline to reflect that change. Take a look at the Sigma rules that you will be using and do a spot check of the rules compared to the data you've got and make sure that the fields directly line up or that you've got a mapping written in your pipeline. Since I used Sysmon to capture this data, there should not be much of a deviation. 

Before we move on there is one last thing you need to check in the pipeline: make sure that the `postprocessing` or `finalizers` section defines the `dispatch.index_earliest` field with some large value like `-1y`. I do not recommend doing this against production data, but in our instance we need to make sure that the small amount of data that I generated for this lab will actually trigger detections. By using `index_earliest` we are defining the timestamp that this data was written to the index which is opposed to the normal `_time` (and the associated `earliest_time` search modifier) field that you see in your logs which is when the event was generated on the endpoint.

Finally we need to review our cron task and make sure that it is running as expected. If you designed your cron task to run once every 24 hours, feel free to go touch some grass. If you're impatient you can just run the script manually and let your rules get populated inside of Splunk. Based on how you defined the `cron_schedule` in your pipeline for each rule you can again either go touch some grass or execute the some of the searches manually. (Settings -> Searches, reports, and alerts -> Set `App:` to our Sigma app and `Owner:` to all -> hit `Run`)

If your pipeline matches mine and it is using `| collect index=notable_events`, confirm that you have created the `notable_events` index before you run any rules. Once you've got detections firing you will see events in this index that you can use to alert your SOC of potential security events!

## Reviewing Events

Splunk does some weird things with the `| collect` command. For one, it fails to properly parse mutlivalue fields unless you modify your limits.conf file. Additionally if you modify the `sourcetype` field with the collect command, it will count against your Splunk licensing metrics. Additionally it sometimes has a hard time pushing fields that we have added in the search with an `| eval` command prior to the `| collect` command because it prefers to just grab the `_raw` field and push that to the collection index. There are ways around all of these issues and you are welcome to modify your Splunk configuration and your pipeline to best fit your needs. In a typical enterprise environment you can use a notable events index like this one to feed directly into your ticketing or SOAR system. Another action you should consider is using the the `action.<action_name>` commands from the savedsearches.conf configuration that will perform functions in Splunk such as send an email or populate a lookup. These actions sometimes require licensing or configuration that is outside of the free splunk instance we are using, but if you are so lucky to have a license or the enterprise security suite, please play around with these functions. You can modify your pipeline file in the `finalizers` section or in your `postprocessing` section.

For the sake of this course, we will ignore those issues and features and just look at the results of our events. Navigate to `Settings -> Searches, reports, and alerts`, select your Sigma app and change `Owner` to `All`. Sort by `Alerts` and by now you should have a few in here. Pick one and click `View Recent` to view the times that this search has been executed. Select one of the results on this page to view the actual search that was executed and the results from that search. In our lab, this is the view that you as a now temporary SOC analyst will be reviewing. You can see that the `search_description` was added as a field directly from the Sigma rule that we ran and you can use this information to start your incident response process.